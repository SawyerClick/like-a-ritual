{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "## AN AUDIOVISUAL ODE TO TITLE FIGHT\n",
    "## USING SPOTIFY DATA TO FIGURE OUT WHY TITLE FIGHT IS A SCENE FAVORITE YEARS AFTER THEIR LAST ALBUM RELEASE\n",
    "## BY SAWYER CLICK\n",
    "\n",
    "#imports\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.notebook import tqdm\n",
    "import requests, re, string, random, os, base64, requests, json, nltk\n",
    "\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "stop_words = stopwords.words('english')\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "load_dotenv()\n",
    "pd.set_option('display.max_colwidth', 999)\n",
    "\n",
    "\n",
    "# get tokens\n",
    "spotify_client_id = os.getenv(\"tfID\")\n",
    "spotify_client_secret = os.getenv(\"tfSecret\")\n",
    "token = os.getenv(\"SPOTIFY_ACCESS_TOKEN\")\n",
    "genius_token = os.getenv(\"GENIUS_API\")\n",
    "\n",
    "# set up how we use the tokens\n",
    "access_token = os.getenv(\"SPOTIFY_ACCESS_TOKEN\")\n",
    "raw_token = !spotify-token\n",
    "token = f\"Bearer {raw_token[1].replace('Your token is:  ','')}\"\n",
    "base_url = \"https://api.spotify.com/v1\"\n",
    "headers = {'Authorization': token}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab artist data for title fight\n",
    "tfURI = '2CnhqfjUG0qzsru0SMuhrk'\n",
    "search_url = f\"{base_url}/artists/{tfURI}\"\n",
    "params = {'limit':1}\n",
    "artist = requests.get(search_url, params=params, headers=headers).json()\n",
    "\n",
    "with open('artist.json', 'w') as outfile:\n",
    "    json.dump(artist, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting uris for these albums & singles: ['Hyperview', 'Floral Green', 'Shed', 'The Last Thing You Forget', 'Spring Songs', 'Face Ghost', \"Flood of '72\"]\n"
     ]
    }
   ],
   "source": [
    "# getting album data\n",
    "search_url = f\"{base_url}/artists/{tfURI}/albums\"\n",
    "params = {'limit':50, 'include_groups': 'album,single'}\n",
    "raw_albums = requests.get(search_url, params=params, headers=headers).json()\n",
    "\n",
    "# limiting to US market to remove duplicate albums with one-off markets\n",
    "albumsBase = []\n",
    "for album in raw_albums['items']:\n",
    "    if 'US' in album['available_markets']:\n",
    "        del album['available_markets']\n",
    "        albumsBase.append(album)\n",
    "\n",
    "with open('albumsBase.json', 'w') as outfile:\n",
    "    json.dump(albumsBase, outfile)\n",
    "    \n",
    "print('Collecting uris for these albums & singles:', [album['name'] for album in albumsBase])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# baseSearchAlbum = f'{base_url}/albums/'\n",
    "# albums = []\n",
    "# tracksBase = []\n",
    "# for album in albumsBase:\n",
    "    \n",
    "#     #store data from this album in our albums array\n",
    "#     thisAlbum = requests.get(baseSearchAlbum+album['uri'].split(':')[2], params={'limit':1}, headers=headers).json()\n",
    "#     del thisAlbum['available_markets']\n",
    "#     albums.append(thisAlbum)\n",
    "    \n",
    "#     # create a function to grab all the tracks and get their base data\n",
    "#     # eventually add endpoints for collecting anaylsis\n",
    "#     albumTracks = requests.get(thisAlbum['tracks']['href'], headers=headers).json()\n",
    "#     for track in albumTracks['items']:\n",
    "#         # attach the album name\n",
    "#         track['album'] = album['name']\n",
    "#         track['popularity'] = requests.get('https://api.spotify.com/v1/tracks/'+track['id'], headers=headers).json()['popularity']\n",
    "#         # delete the available markets array bc it takes up so much space\n",
    "#         del track['available_markets']\n",
    "#         tracksBase.append(track)\n",
    "        \n",
    "# trackURIs = [track['uri'].split(':')[2] for track in tracksBase]\n",
    "        \n",
    "# with open('albums.json', 'w') as outfile:\n",
    "#     json.dump(albums, outfile)\n",
    "    \n",
    "# # grab the features of each track in their albums\n",
    "# tracksFt = []\n",
    "# [tracksFt.append(requests.get('https://api.spotify.com/v1/audio-features/'+uri, headers=headers).json()) for uri in trackURIs]\n",
    "# # print(f'Grabbed data for {len(trackURIs)} tracks')\n",
    "\n",
    "# for track in tracksBase:\n",
    "#     for ftTrack in tracksFt:\n",
    "#         if track['uri'] == ftTrack['uri']:\n",
    "#             track['features'] = ftTrack\n",
    "            \n",
    "# with open('tracks.json', 'w') as outfile:\n",
    "#     json.dump(tracksBase, outfile)\n",
    "    \n",
    "# print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tracks.json') as tracksRaw:\n",
    "    tracks = json.load(tracksRaw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Genius for song lyrics\n",
    "Then some sentiment analysis with nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5925c167619348b79c3a222fd474c3af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# holding on to the spotify id so i can join later\n",
    "gSearchTracks = [{'name':track['name'], 'id':track['id']} for track in tracks]\n",
    "gData = []\n",
    "\n",
    "gtoken = os.getenv('GENIUS_API')\n",
    "gid = '53688'\n",
    "gheaders = {'Authorization': f\"Bearer {gtoken}\"}\n",
    "gbase = 'https://api.genius.com/'\n",
    "gsite = \"http://genius.com\"\n",
    "gartist = gbase + 'artists/' + gid\n",
    "gsong = gbase + 'songs/'\n",
    "gsearch = gbase+'search/'\n",
    "gparams = {'q': 'Hypernight Title Fight'}\n",
    "\n",
    "for track in tqdm(gSearchTracks):\n",
    "    searchThis = track['name']+' Title Fight'\n",
    "    response = requests.get(gsearch, params={'q':searchThis}, headers=gheaders).json()\n",
    "    for hit in response['response']['hits']:\n",
    "        if hit[\"result\"][\"primary_artist\"][\"id\"] == int(gid):\n",
    "            page = requests.get(gsite + hit['result']['path'])\n",
    "            html = BeautifulSoup(page.text, \"html.parser\")\n",
    "            [h.extract() for h in html('script')]\n",
    "            lyrics = html.find(\"div\", class_=\"lyrics\").get_text()\n",
    "            track['lyricsRaw'] = lyrics\n",
    "            track['lyrics'] = lyrics.replace('\\n', ' ')\n",
    "            track['tokenized'] = nltk.word_tokenize(lyrics.replace('\\n', ' ')), stop_words\n",
    "            \n",
    "\n",
    "            gData.append(track)\n",
    "            break\n",
    "            \n",
    "with open('trackLyrics.json', 'w') as outfile:\n",
    "    json.dump(gData, outfile)\n",
    "\n",
    "with open('trackLyrics.json') as lyricsRaw:\n",
    "    lyrics = json.load(lyricsRaw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using nltk to get percent pos, neg, neu\n",
    "for track in lyrics:\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    neu = 0\n",
    "    for line in track['lyricsRaw'].split('\\n'):\n",
    "        comp = sid.polarity_scores(line)\n",
    "        comp = comp['compound']\n",
    "        if comp >= .2:\n",
    "            pos += 1\n",
    "        elif comp > -.2 and comp < .2:\n",
    "            neu += 1\n",
    "        else:\n",
    "            neg += 1\n",
    "            \n",
    "    total = neg + neu + pos\n",
    "    track['perNeg'] = (neg/float(total))*100\n",
    "    track['perNeu'] = (neu/float(total))*100\n",
    "    track['perPos'] = (pos/float(total))*100\n",
    "    \n",
    "with open('trackLyrics.json', 'w') as outfile:\n",
    "    json.dump(lyrics, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Face Ghost']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## have lyrics for all songs except for Face Ghost\n",
    "list(set([track['name'] for track in tracks]).difference([track['name'] for track in lyrics]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,trackT in enumerate(tracks):\n",
    "    for lyricT in lyrics:\n",
    "        if trackT['id'] == lyricT['id']:\n",
    "            trackT['lyricsRaw'] = lyricT['lyricsRaw']\n",
    "            trackT['lyrics'] = lyricT['lyrics']\n",
    "            trackT['perNeg'] = lyricT['perNeg']\n",
    "            trackT['perNeu'] = lyricT['perNeu']\n",
    "            trackT['perPos'] = lyricT['perPos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tracks.json', 'w') as outfile:\n",
    "    json.dump(tracks, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
